主要工作是文献调研。





一些感悟

1. 很多论文感觉并未做出特别明显的，或者说，很创新的贡献，但是却能发表，我觉得主要原因就是使用结合的方法：将FL与一些应用场景结合，做一些普遍的理论推导和实验。而这些应用场景常常都是机器学习中的其他领域。总的来说就是其他领域的问题加上FL就感觉可以发论文了。不过这个缺少创新性，如果作为自己硕士论文答辩，底气不足，思维不够开阔。比如强化学习加FL，我看到太多了，论文比较公式化，创新的地方主要是公式，不过我觉得很多没有抓住问题的核心。

一些初步想法

将FL作为优化搜索问题，往演化算法方向靠，并且研究**梯度**，搜索梯度，去获得一个很好的解。以下是我的一些算法核心思想。

```c++
1. select 根据调研的论文，从客户端选择入手这是比较常见的。我的想法是以贡献值进行选择并且是以某种概率选择。贡献值又该如何定义？（数据集大小？数据新鲜度？模型受训练新鲜度？parameter抖动的大小？）我们可以参考shaply value，以聚合这个客户端的参数对性能提升的多少作为具体贡献值。不过计算量特别大，另辟蹊径还是进行改良？
2. mutate 参考"FedMut: Generalized Federated Learning via Stochastic Mutation"这篇文章的想法，对于gradient或者parameters进行变异，摆脱局部最优。或者我们直接把gradient作为gene。我们该怎么变异？主要是有启发式的变异才比较好，增加算法效率。以什么作为启发？这是个问题。除此之外，变异的具体操作可以参考动量操作
```



未来工作安排

1. 多看看论文代码，主要学习对于神经网络中的参数的处理。

2. 学习机器学习的基本公式（我发现我对很多论文中出现的公式以及变量不太了解）

3. 着手我的一些想法。